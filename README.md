# Octo

LangGraph multi-agent CLI with Rich console UI and Telegram transport.

Octo orchestrates Claude Code agents from multiple projects through a single chat interface. It loads AGENT.md files, connects to MCP servers, and routes tasks to the right agent via a supervisor pattern.

## Quick Start

```bash
pip install -e .
octo init          # interactive setup wizard — creates .env + scaffolds .octo/
octo               # start chatting
```

`octo init` walks you through provider selection, credential entry, and workspace setup. It validates your credentials with a real API call before saving.

**QuickStart mode** (3 prompts — pick provider, paste key, done):

```bash
octo init --quick
```

**Non-interactive** (for CI / Docker):

```bash
ANTHROPIC_API_KEY=sk-ant-... octo init --quick --provider anthropic --no-validate --force
```

## Health Check

```bash
octo doctor        # verify configuration — 8 checks with PASS/FAIL
octo doctor --fix  # re-run setup wizard on failures
octo doctor --json # machine-readable output
```

## Project Structure

```
.env                    # credentials, model config (generated by octo init)
.mcp.json               # MCP server definitions — optional
.octo/                  # workspace state
├── persona/            # SOUL.md, IDENTITY.md, USER.md, AGENTS.md, MEMORY.md, TOOLS.md
├── agents/             # Octo-native agent definitions (AGENT.md per folder)
├── skills/             # skill definitions (SKILL.md per folder)
├── memory/             # daily memory logs (YYYY-MM-DD.md)
├── projects/           # project registry (auto-generated JSON)
├── STATE.md            # human-readable project state
└── octo.db             # conversation checkpoints (SQLite)
octo/                   # Python package
└── wizard/             # setup wizard + health check
```

## Configuration

All config lives in `.env` (generated by `octo init`, or create manually):

```env
# LLM Provider — auto-detected from model name, or set explicitly
# LLM_PROVIDER=bedrock  # anthropic | bedrock | openai | azure

# Model tiers (different agents use different tiers to save costs)
DEFAULT_MODEL=eu.anthropic.claude-sonnet-4-5-20250929-v1:0
HIGH_TIER_MODEL=eu.anthropic.claude-sonnet-4-5-20250929-v1:0
LOW_TIER_MODEL=eu.anthropic.claude-haiku-4-5-20251001-v1:0

# AWS Bedrock
AWS_REGION=eu-central-1
AWS_ACCESS_KEY_ID=...
AWS_SECRET_ACCESS_KEY=...

# Anthropic direct (used when model name starts with "claude-")
# ANTHROPIC_API_KEY=sk-ant-...

# OpenAI (used when model name starts with "gpt-" or "o1-"/"o3-")
# OPENAI_API_KEY=sk-...

# Azure OpenAI
# AZURE_OPENAI_API_KEY=...
# AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
# AZURE_OPENAI_API_VERSION=2024-12-01-preview

# Agent directories — external AGENT.md sources (colon-separated)
AGENT_DIRS=/path/to/project-a/.claude/agents:/path/to/project-b/.claude/agents

# Model profile — quality | balanced | budget
MODEL_PROFILE=balanced

# Telegram (shared thread with console)
TELEGRAM_BOT_TOKEN=...
TELEGRAM_OWNER_ID=...

# Voice (ElevenLabs TTS)
ELEVENLABS_API_KEY=...
ELEVENLABS_VOICE_ID=...
```

## Model Factory

The model factory (`octo/models.py`) auto-detects the provider from the model name:

| Model name pattern | Provider |
|---|---|
| `eu.anthropic.*`, `us.anthropic.*` | AWS Bedrock |
| `claude-*` | Anthropic direct |
| `gpt-*`, `o1-*`, `o3-*` | OpenAI |
| `gpt-*` + `AZURE_OPENAI_ENDPOINT` set | Azure OpenAI |

Override with `LLM_PROVIDER` env var if needed.

## Agents

Agents are loaded from Claude Code `AGENT.md` files (YAML frontmatter + markdown body). Directories scanned:

1. `<workspace>/.claude/agents/`
2. Any paths in `AGENT_DIRS` env var
3. `.octo/agents/*/AGENT.md` (Octo-native agents)

Agents are assigned model tiers based on name:
- **high** — `architect`, `planner`, `rca-autofixer`
- **low** — everything else

## Slash Commands

| Command | Description |
|---|---|
| `/help` | Show commands |
| `/clear` | Reset conversation (new thread) |
| `/compact` | Summarize older messages to free context |
| `/context` | Show context window usage |
| `/agents` | List loaded agents |
| `/skills` | List loaded skills |
| `/projects` | Show project registry |
| `/sessions` | List saved sessions |
| `/plan` | Show current todo list |
| `/profile [name]` | Show/switch model profile (quality/balanced/budget) |
| `/voice on\|off` | Toggle TTS |
| `/model <name>` | Switch model |
| `/thread [id]` | Show or switch thread |
| `/<skill>` | Invoke a skill |
| `exit` | End session |

## CLI Commands

| Command | Description |
|---|---|
| `octo` | Start interactive chat (default) |
| `octo init` | Run setup wizard |
| `octo doctor` | Check configuration health |

## Architecture

```
Console (Rich)  ←→  Supervisor (create_supervisor)  ←→  Agent Pool (AGENT.md)
Telegram Bot    ←↗         ↕                                    ↕
                      MCP Tools (.mcp.json)              MCP Tools (filtered)
                      Todo tools (plan/track)
```

Both console and Telegram share the same conversation thread. Messages from either channel appear in the shared history.
